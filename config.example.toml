# Synapse configuration
# Copy to ~/.config/synapse/config.toml and customize

[spec]
enabled = true
auto_generate = true                   # auto-generate specs from project files
scan_depth = 3                         # max levels to walk up for project files (ignored inside git repos)
discover_from_help = true              # auto-discover specs by running --help on unknown commands
discover_blocklist = []                # commands to never auto-discover

[security]
command_blocklist = ["export *=", "curl -u", "curl -H \"Authorization*\""]

[llm]
enabled = true                         # enable LLM-powered features (spec discovery, NL translation)
provider = "openai"                    # only supported provider (also used for local OpenAI-compatible endpoints)
api_key_env = "LMSTUDIO_API_KEY"       # env var name containing the API key (placeholder is accepted for local endpoints)
base_url = "http://127.0.0.1:1234"    # API base URL (default: LM Studio local endpoint)
model = "gpt-4o-mini"                  # model to use
timeout_ms = 10000                     # per-request timeout
natural_language = true                # enable ? prefix mode for NL -> command translation
nl_max_suggestions = 3                 # number of alternative commands to generate for NL queries

# LM Studio (local) example:
# [llm]
# enabled = true
# provider = "openai"
# api_key_env = "LMSTUDIO_API_KEY"
# base_url = "http://127.0.0.1:1234"
# model = "qwen2.5-coder-7b-instruct-mlx"

[completions]
# output_dir = "~/.synapse/completions"              # override output directory
