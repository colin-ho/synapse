# Synapse configuration
# Copy to ~/.config/synapse/config.toml and customize

[general]
# socket_path = "/tmp/synapse.sock"     # auto-detected if omitted
debounce_ms = 150                       # AI trigger delay (ms)
max_suggestion_length = 200             # truncate long suggestions
accept_key = "right-arrow"              # or "tab"
log_level = "warn"                      # "error" | "warn" | "info" | "debug" | "trace"

[history]
enabled = true
max_entries = 50000
fuzzy = true                            # enable fuzzy matching when prefix fails

[context]
enabled = true
scan_depth = 3                          # max levels to walk up (ignored inside git repos)

[ai]
enabled = true
provider = "ollama"                     # "ollama" | "anthropic" | "openai"
model = "llama3"                        # model name
endpoint = "http://localhost:11434"      # for ollama; ignored for anthropic/openai
api_key_env = "ANTHROPIC_API_KEY"       # env var name for API providers
max_tokens = 50
temperature = 0.0
timeout_ms = 2000                       # give up after this (ms)
fallback_to_local = true                # if API fails, skip AI layer silently
rate_limit_rpm = 30                     # max API requests per minute
max_concurrent_requests = 2             # max in-flight API requests

[weights]
# Weights are normalized to sum to 1.0
history = 0.35
context = 0.2
ai = 0.3
recency = 0.15

[security]
scrub_paths = true                      # redact home directory in API payloads
scrub_env_keys = ["*_KEY", "*_SECRET", "*_TOKEN", "*_PASSWORD", "*_CREDENTIALS"]
command_blocklist = ["export *=", "curl -u", "curl -H \"Authorization\""]

[logging]
interaction_log = "~/.local/share/synapse/interactions.jsonl"
max_log_size_mb = 50                    # rotate after this size
